<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-08-17T09:02:47-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Gentopia-AI</title><subtitle>The Collective Growth of Intelligent Agents</subtitle><entry><title type="html">Motivation</title><link href="http://localhost:4000/blog/sample-post" rel="alternate" type="text/html" title="Motivation" /><published>2020-04-24T00:00:00-04:00</published><updated>2020-04-24T00:00:00-04:00</updated><id>http://localhost:4000/blog/sample-post</id><content type="html" xml:base="http://localhost:4000/blog/sample-post"><![CDATA[<p>Agent practitioners start to realize the difficulty in tuning a ‚Äúwell-rounded‚Äù agent with tons of tools or instructions in a single layer. Recent studies like TinyStories, Specializing Reasoning, Let‚Äôs Verify SbS, ReWOO, etc. also point us towards an intuitive yet undervalued direction üëâ</p>

<p><strong>An LLM is more capable if you create a context/distribution shift specialized to some target tasks.</strong></p>

<p>Sadly, there is no silver bullet for agent specialization. For example, you can:</p>
<ul>
  <li>Simply add Let‚Äôs think step by step. in your prompt for more accurate Math QA.</li>
  <li>Give a few-shot exemplar in your prompt to guide a better reasoning trajectory for novel plotting.</li>
  <li>Supervise fine-tuning (SFT) your 70B llama2 like this to match reasoning of 175B GPT-3.5.</li>
  <li>Tune your agent paradigm like this demo to easily half the execution time for Seach &amp; Summarize.</li>
</ul>

<p>Isn‚Äôt it beautiful if one shares his effort in specialized intelligence, allowing others to reproduce, build on, or interact with it? ü§ó This belief inspires us to build Gentopia, designed for agent specialization, sharing, and interaction, to stackingly achieve collective growth towards greater intelligence..</p>]]></content><author><name></name></author><category term="motivation" /><summary type="html"><![CDATA[The motivation of Gentopia.]]></summary></entry><entry><title type="html">How to Install and use Gentopia?</title><link href="http://localhost:4000/blog/how-to-install-whatatheme" rel="alternate" type="text/html" title="How to Install and use Gentopia?" /><published>2020-04-22T00:00:00-04:00</published><updated>2020-04-22T00:00:00-04:00</updated><id>http://localhost:4000/blog/how-to-install-whatatheme</id><content type="html" xml:base="http://localhost:4000/blog/how-to-install-whatatheme"><![CDATA[<h1 id="installation">Installation</h1>
<p>Please follow the instructions below to set up Gentopia and GentPool on your system.
We recommend using a virtual environment.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda create <span class="nt">--name</span> gentenv <span class="nv">python</span><span class="o">=</span>3.10
conda activate gentenv
</code></pre></div></div>

<h2 id="install-gentopia-from-pypi">Install Gentopia from PyPI</h2>
<p>To install the basic framework,</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>gentopia
</code></pre></div></div>
<p>Additionally, if you want to use open LLMs like <code class="language-plaintext highlighter-rouge">llama</code> on huggingface, together with 8-bit/4-bit quantization tricks, install with</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>gentopia[huggingface]
</code></pre></div></div>
<p>NOTE:  We are still in early development (gentopia <code class="language-plaintext highlighter-rouge">v0.x.x</code> is considered early access), you may want to frequently</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install</span> <span class="nt">--upgrade</span> gentopia
</code></pre></div></div>

<h2 id="install-gentopia-from-source">Install Gentopia from source</h2>
<p>Alternatively, if you are a true hacker who modifies the source code frequently, we recommend installing from source.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone git@github.com:Gentopia-AI/Gentopia.git
<span class="nb">cd </span>Gentopia
pip <span class="nb">install</span> <span class="nt">-e</span> <span class="nb">.</span>
</code></pre></div></div>

<h2 id="install-gentpool">Install GentPool</h2>
<p>We recommend using GentPool as a space to build your agent because you can easily call other public agents for interaction, 
and access our unique benchmark eval to test your agent.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone git@github.com:Gentopia-AI/GentPool.git
</code></pre></div></div>
<p>Create a .env file under GentPool (ignored by git) and put your API Keys inside. They will be registered as environmental variables at run time.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>GentPool
<span class="nb">touch</span> .env
<span class="nb">echo</span> <span class="s2">"OPENAI_API_KEY=&lt;your_openai_api_key&gt;"</span> <span class="o">&gt;&gt;</span> .env
<span class="nb">echo</span> <span class="s2">"WOLFRAM_ALPHA_APPID=&lt;your_wolfram_alpha_api_key&gt;"</span> <span class="o">&gt;&gt;</span> .env
</code></pre></div></div>
<p>‚Ä¶ and so on if you plan to use other service keys.</p>

<h2 id="download-public-gentbench-data">Download public GentBench data</h2>

<p>GentBench is our unique benchmark eval for agents. It tests a wide range of agent capability beyond vanilla LLMs.
See <a href="">here</a> for more details.</p>

<p>GentBench is half-public and half-private (both will be updated and expanded). 
You have full access to the public data for testing, fine-tuning or so, but private benchmark will only be used to evaluate agents registered in GentPool.
This prevents overfitting and gives you a sense of generalizability.
To download the public benchmark, make sure you‚Äôve installed <a href="https://git-lfs.com/">Git-LFS</a> and GentPool, then</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>GentPool
git lfs fetch <span class="nt">--all</span>
git lfs pull
</code></pre></div></div>
<p>Then you will see downloaded tasks under <code class="language-plaintext highlighter-rouge">GentPool/benchmark/</code>.</p>]]></content><author><name></name></author><category term="how to" /><category term="setup" /><summary type="html"><![CDATA[This post will guide you to install Gentopia, follow the easy steps to set it up.]]></summary></entry><entry><title type="html">How to create agents?</title><link href="http://localhost:4000/blog/what-is-jekyll-how-to-use-it" rel="alternate" type="text/html" title="How to create agents?" /><published>2020-04-21T00:00:00-04:00</published><updated>2020-04-21T00:00:00-04:00</updated><id>http://localhost:4000/blog/what-is-jekyll-how-to-use-it</id><content type="html" xml:base="http://localhost:4000/blog/what-is-jekyll-how-to-use-it"><![CDATA[<h1 id="quick-start">Quick Start</h1>
<p>The best way to get started with Gentopia is to go through our short demo <a href="https://www.youtube.com/playlist?list=PLZ2Bx-k4vH2dyvW9md5SHx4GBQOGoIJWr">tutorials</a>.</p>

<p>Grab one of the following agent template and store it in <code class="language-plaintext highlighter-rouge">agent.yaml</code>, then jump to <a href="quick_start.md#howto-assemble-your-agent">Assembling Agents</a> section. 
See <a href="agent_components.md">Agent Components</a> for custom configurations.</p>

<h2 id="vanilla-agent">Vanilla Agent</h2>
<p><code class="language-plaintext highlighter-rouge">vanilla</code> agents are plain LLMs instances without any plugin. 
It is useful if you simply want to create context shift, such as granting identities or instructing an LLM to behave in a particular way.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Vanilla agent template</span>
<span class="na">name</span><span class="pi">:</span> <span class="s">vanilla_template</span>
<span class="na">version</span><span class="pi">:</span> <span class="s">0.0.1</span>
<span class="na">type</span><span class="pi">:</span> <span class="s">vanilla</span>
<span class="na">description</span><span class="pi">:</span> <span class="s">A plain gpt-4 LLM.</span>
<span class="na">target_tasks</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">anything to do with an LLM</span> 
<span class="na">llm</span><span class="pi">:</span>
  <span class="na">model_name</span><span class="pi">:</span> <span class="s">gpt-4</span>
  <span class="na">params</span><span class="pi">:</span>
    <span class="na">temperature</span><span class="pi">:</span> <span class="m">0.0</span>
    <span class="na">top_p</span><span class="pi">:</span> <span class="m">0.9</span>
    <span class="na">repetition_penalty</span><span class="pi">:</span> <span class="m">1.0</span>
    <span class="na">max_tokens</span><span class="pi">:</span> <span class="m">1024</span>
<span class="na">prompt_template</span><span class="pi">:</span> <span class="kt">!prompt</span> <span class="s">VanillaPrompt</span>

</code></pre></div></div>

<h2 id="react-agent">ReAct Agent</h2>
<p><code class="language-plaintext highlighter-rouge">react</code> agents are by far the most common type of agents. It prompts LLMs to respond in multiple ‚ÄúThought -&gt; Action -&gt; Observation‚Äù cycles.
A stop word ‚ÄúObservation: ‚Äú is used to detect plugin calls, and ‚ÄúFinal Answer: ‚Äú is used to detect cycle end.
It prompts LLM at each tool-call with all previous history stored in a ‚Äúscratchpad‚Äù.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ReAct Agent Template</span>
<span class="na">name</span><span class="pi">:</span> <span class="s">react_template</span>
<span class="na">version</span><span class="pi">:</span> <span class="s">0.0.1</span>
<span class="na">type</span><span class="pi">:</span> <span class="s">react</span>
<span class="na">description</span><span class="pi">:</span> <span class="s">A react agent capable of online web search and browsing.</span>
<span class="na">target_tasks</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">web search</span>
  <span class="pi">-</span> <span class="s">web browsing</span>
<span class="na">llm</span><span class="pi">:</span>
  <span class="na">model_name</span><span class="pi">:</span> <span class="s">gpt-4</span>
  <span class="na">params</span><span class="pi">:</span>
    <span class="na">temperature</span><span class="pi">:</span> <span class="m">0.0</span>
    <span class="na">top_p</span><span class="pi">:</span> <span class="m">0.9</span>
    <span class="na">repetition_penalty</span><span class="pi">:</span> <span class="m">1.0</span>
    <span class="na">max_tokens</span><span class="pi">:</span> <span class="m">1024</span>
<span class="na">prompt_template</span><span class="pi">:</span> <span class="kt">!prompt</span> <span class="s">ZeroShotReactPrompt</span>
<span class="na">plugins</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">google_search</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">web_page</span>
</code></pre></div></div>

<h2 id="openai-function-agent">OpenAI Function Agent</h2>
<p><code class="language-plaintext highlighter-rouge">openai</code> agents are powered by <a href="https://openai.com/blog/function-calling-and-other-api-updates">OpenAI function-call API</a>.
It uses a similar sequential style as <code class="language-plaintext highlighter-rouge">react</code> agents, interleaving LLM response and tool-calls. 
According to OpenAI, it is syntax fine-tuned to generate function call message.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># OpenAI agent template</span>
<span class="na">name</span><span class="pi">:</span> <span class="s">openai_template</span>
<span class="na">version</span><span class="pi">:</span> <span class="s">0.0.1</span>
<span class="na">type</span><span class="pi">:</span> <span class="s">openai</span>
<span class="na">description</span><span class="pi">:</span> <span class="s">An openai agent capable of online web search and browsing.</span>
<span class="na">target_tasks</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">web search</span>
  <span class="pi">-</span> <span class="s">web browsing</span>
<span class="na">llm</span><span class="pi">:</span>
  <span class="na">model_name</span><span class="pi">:</span> <span class="s">gpt-4-0613</span>
  <span class="na">params</span><span class="pi">:</span>
    <span class="na">temperature</span><span class="pi">:</span> <span class="m">0.0</span>
    <span class="na">top_p</span><span class="pi">:</span> <span class="m">0.9</span>
    <span class="na">repetition_penalty</span><span class="pi">:</span> <span class="m">1.0</span>
    <span class="na">max_tokens</span><span class="pi">:</span> <span class="m">1024</span>
<span class="na">prompt_template</span><span class="pi">:</span> <span class="kt">!prompt</span> <span class="s">VanillaPrompt</span>
<span class="na">plugins</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">google_search</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">web_page</span>
</code></pre></div></div>

<h2 id="rewoo-agent">ReWOO Agent</h2>
<p><code class="language-plaintext highlighter-rouge">rewoo</code> agents are designed to improve efficiency of sequential agent types by prompting LLM to plan ahead and use hypothetical
tool responses to complete the reasoning process. Then the algorithm parses that ‚Äúblueprint‚Äù into a Directed Acyclic Graph (DAG)
and executes it in parallel with topological order.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ReWOO agent template</span>
<span class="na">name</span><span class="pi">:</span> <span class="s">rewoo_template</span>
<span class="na">version</span><span class="pi">:</span> <span class="s">0.0.1</span>
<span class="na">type</span><span class="pi">:</span> <span class="s">rewoo</span>
<span class="na">description</span><span class="pi">:</span> <span class="s">A rewoo agent capable of online web search and browsing.</span>
<span class="na">target_tasks</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">web search</span>
  <span class="pi">-</span> <span class="s">web browsing</span>
<span class="na">llm</span><span class="pi">:</span>
  <span class="na">Planner</span><span class="pi">:</span>
    <span class="na">model_name</span><span class="pi">:</span> <span class="s">gpt-4</span>
    <span class="na">params</span><span class="pi">:</span>
      <span class="na">temperature</span><span class="pi">:</span> <span class="m">0.0</span>
      <span class="na">top_p</span><span class="pi">:</span> <span class="m">0.9</span>
      <span class="na">repetition_penalty</span><span class="pi">:</span> <span class="m">1.0</span>
      <span class="na">max_tokens</span><span class="pi">:</span> <span class="m">1024</span>
  <span class="na">Solver</span><span class="pi">:</span>
    <span class="na">model_name</span><span class="pi">:</span> <span class="s">gpt-3.5-turbo</span>
    <span class="na">params</span><span class="pi">:</span>
      <span class="na">temperature</span><span class="pi">:</span> <span class="m">0.0</span>
      <span class="na">top_p</span><span class="pi">:</span> <span class="m">0.9</span>
      <span class="na">repetition_penalty</span><span class="pi">:</span> <span class="m">1.0</span>
      <span class="na">max_tokens</span><span class="pi">:</span> <span class="m">1024</span>
<span class="na">prompt_template</span><span class="pi">:</span>
  <span class="na">Planner</span><span class="pi">:</span> <span class="kt">!prompt</span> <span class="s">ZeroShotPlannerPrompt</span>
  <span class="na">Solver</span><span class="pi">:</span> <span class="kt">!prompt</span> <span class="s">ZeroShotSolverPrompt</span>
<span class="na">plugins</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">google_search</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">web_page</span>
</code></pre></div></div>

<h2 id="openai-function-agent-with-memory">OpenAI Function Agent (with Memory)</h2>
<p><code class="language-plaintext highlighter-rouge">openai_memory</code> agents are specialized <code class="language-plaintext highlighter-rouge">openai</code> agents enabling memory retrieval from vector database.
Abusing index retrieval is highly expensive and lagging, so we generally don‚Äôt encourage it unless you expect unavoidably long context. 
After tons of tuning, we present this novel <a href="">2-level memory</a> architecture for fast and robust memmory retrieval.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># OpenAI agent template</span>
<span class="na">name</span><span class="pi">:</span> <span class="s">openai_memory_template</span>
<span class="na">version</span><span class="pi">:</span> <span class="s">0.0.1</span>
<span class="na">type</span><span class="pi">:</span> <span class="s">openai_memory</span>
<span class="na">description</span><span class="pi">:</span> <span class="s">An openai memory agent capable of online web search and browsing in unlimited context length.</span>
<span class="na">target_tasks</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">web search</span>
  <span class="pi">-</span> <span class="s">web browsing</span>
<span class="na">llm</span><span class="pi">:</span>
  <span class="na">model_name</span><span class="pi">:</span> <span class="s">gpt-4-0613</span>
  <span class="na">params</span><span class="pi">:</span>
    <span class="na">temperature</span><span class="pi">:</span> <span class="m">0.0</span>
    <span class="na">top_p</span><span class="pi">:</span> <span class="m">0.9</span>
    <span class="na">repetition_penalty</span><span class="pi">:</span> <span class="m">1.0</span>
    <span class="na">max_tokens</span><span class="pi">:</span> <span class="m">1024</span>
<span class="na">prompt_template</span><span class="pi">:</span> <span class="kt">!prompt</span> <span class="s">VanillaPrompt</span>
<span class="na">plugins</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">google_search</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">web_page</span>
<span class="na">memory</span><span class="pi">:</span>
  <span class="na">memory_type</span><span class="pi">:</span> <span class="s">chroma</span>
  <span class="na">conversation_threshold</span><span class="pi">:</span> <span class="m">2</span>      <span class="c1"># first-level memory</span>
  <span class="na">reasoning_threshold</span><span class="pi">:</span> <span class="m">2</span>         <span class="c1"># second-level memory</span>
  <span class="na">params</span><span class="pi">:</span>
    <span class="na">index</span><span class="pi">:</span> <span class="s">main</span>
    <span class="na">top_k</span><span class="pi">:</span> <span class="m">2</span>
</code></pre></div></div>

<h2 id="howto-assemble-your-agent">Howto: Assemble your Agent</h2>
<p>While it‚Äôs recommended to use Gentopia in <a href="gentpool.md">GentPool</a>, you may follow instructions below to run in your own python environment:</p>

<p>Suppose you have saved your agent config to <code class="language-plaintext highlighter-rouge">my_agent.yaml</code>, make sure you have set up <code class="language-plaintext highlighter-rouge">.env</code> environmental keys similar to <a href="installation.md#install-gentpool">here</a>. Then to chat with your agent in our built-in streaming CLI, run:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">dotenv</span>
<span class="kn">from</span> <span class="nn">gentopia</span> <span class="kn">import</span> <span class="n">chat</span>
<span class="kn">from</span> <span class="nn">gentopia.assembler.agent_assembler</span> <span class="kn">import</span> <span class="n">AgentAssembler</span>

<span class="n">dotenv</span><span class="p">.</span><span class="n">load_dotenv</span><span class="p">(</span><span class="s">".env"</span><span class="p">)</span>  <span class="c1"># load environmental keys
</span><span class="n">agent</span> <span class="o">=</span> <span class="n">AgentAssembler</span><span class="p">(</span><span class="nb">file</span><span class="o">=</span><span class="s">'my_agent.yaml'</span><span class="p">).</span><span class="n">get_agent</span><span class="p">()</span>

<span class="n">chat</span><span class="p">(</span><span class="n">agent</span><span class="p">)</span> 
</code></pre></div></div>
<p>To run that agent in your own app</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># AgentOutput contains response content, running cost and token usgae.
</span><span class="n">agent</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="s">"Your instruction or message."</span><span class="p">)</span>
</code></pre></div></div>
<p>To stream output in your app, inherit <a href="https://github.com/Gentopia-AI/Gentopia/blob/c26370cf8c9642d2ec101c2d827b56341058b482/gentopia/output/base_output.py#L12">BaseOutput</a>
with your own output handler</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">oh</span> <span class="o">=</span> <span class="n">Your_OutputHandler</span><span class="p">()</span>
<span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">oh</span><span class="p">,</span> <span class="n">BaseOutput</span><span class="p">)</span>
<span class="c1"># Response content streams into your output handler.
</span><span class="n">agent</span><span class="p">.</span><span class="n">stream</span><span class="p">(</span><span class="s">"Your instruction or message."</span><span class="p">,</span> <span class="n">output_handler</span><span class="o">=</span><span class="n">oh</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="create" /><category term="agent" /><summary type="html"><![CDATA[The quick start on how to create your own agent.]]></summary></entry></feed>